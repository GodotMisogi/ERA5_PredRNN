After initialize model, args.img_channel:30
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012000_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012001_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012002_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012003_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012004_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012005_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012006_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012007_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012008_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012009_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012010_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012011_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012012_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012013_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012014_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012015_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012016_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012017_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012018_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012019_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012000_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012001_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012002_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012003_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012004_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012005_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012006_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012007_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012008_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012009_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012010_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012011_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012012_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012013_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012014_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012015_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012016_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012017_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012018_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012019_3_24hr.npz
train_data_files nums: 40
mnist test iterator, Loading data from /scratch/09012/haoli1/ERA5/val_dataset/era5_train_07012020_3_24hr.npz
NaN value num: 0
mnist train iterator, Loading data from /scratch/09012/haoli1/ERA5/dataset/era5_train_09012006_3_24hr.npz
NaN value num: 0
Iteration: 1, ims.shape: (3, 48, 3, 720, 1440)
The modified gen p mean: 0.44772815704345703, gen p mean: 0.00041758647421374917, mean p set: 0.44830384850502014
loss_pred:0.16042274236679077, decouple_loss:0.08806394785642624
The modified gen p mean: 0.44857311248779297, gen p mean: 0.0005481642438098788, mean p set: 0.449150025844574
loss_pred:0.17107956111431122, decouple_loss:0.08776390552520752
The modified gen p mean: 0.4484907388687134, gen p mean: 0.0007072832086123526, mean p set: 0.4490663409233093
loss_pred:0.16631895303726196, decouple_loss:0.08837810903787613
/work/09012/haoli1/ls6/Pyt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Iteration: 2, ims.shape: (3, 48, 3, 720, 1440)
The modified gen p mean: 0.4487093687057495, gen p mean: 0.0005966551252640784, mean p set: 0.4492858350276947
loss_pred:0.172943115234375, decouple_loss:0.24937796592712402
The modified gen p mean: 0.449130654335022, gen p mean: 0.0005361895309761167, mean p set: 0.4497077465057373
loss_pred:0.17881260812282562, decouple_loss:0.2482946217060089
The modified gen p mean: 0.4475904703140259, gen p mean: 0.0004965348052792251, mean p set: 0.4481671154499054
loss_pred:0.16982778906822205, decouple_loss:0.2479146122932434
Traceback (most recent call last):
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/run1.py", line 338, in <module>
    train_wrapper(model)
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/run1.py", line 277, in train_wrapper
    trainer.train(model, ims, real_input_flag, args, itr)
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/core/trainer.py", line 25, in train
    cost = model.train(ims, real_input_flag)
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/core/models/model_factory_multiGPU.py", line 63, in train
    loss.mean().backward()
  File "/work/09012/haoli1/ls6/Pyt/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/work/09012/haoli1/ls6/Pyt/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 3.61 GiB (GPU 0; 39.42 GiB total capacity; 32.90 GiB already allocated; 3.14 GiB free; 34.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF