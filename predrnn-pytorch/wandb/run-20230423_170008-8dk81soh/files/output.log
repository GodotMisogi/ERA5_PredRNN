After initialize model, args.img_channel:3
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012000_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012001_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012002_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012003_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012004_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012005_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012006_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012007_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012008_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012009_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012010_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012011_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012012_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012013_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012014_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012015_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012016_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012017_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012018_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_07012019_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012000_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012001_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012002_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012003_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012004_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012005_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012006_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012007_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012008_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012009_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012010_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012011_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012012_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012013_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012014_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012015_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012016_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012017_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012018_3_24hr.npz
/scratch/09012/haoli1/ERA5/dataset/era5_train_09012019_3_24hr.npz
train_data_files nums: 40
mnist test iterator, Loading data from /scratch/09012/haoli1/ERA5/val_dataset/era5_train_07012020_3_24hr.npz
NaN value num: 0
mnist train iterator, Loading data from /scratch/09012/haoli1/ERA5/dataset/era5_train_07012005_3_24hr.npz
NaN value num: 0
Iteration: 1, ims.shape: (3, 48, 3, 720, 1440)
The modified gen p mean: 0.44756248593330383, gen p mean: 0.00029863062081858516, mean p set: 0.44804278016090393
loss_pred:0.16779258847236633, decouple_loss:0.09134602546691895
The modified gen p mean: 0.4487855136394501, gen p mean: 0.00023689403315074742, mean p set: 0.44926580786705017
loss_pred:0.16736504435539246, decouple_loss:0.09190882742404938
The modified gen p mean: 0.44869399070739746, gen p mean: 0.0005346493562683463, mean p set: 0.44917428493499756
loss_pred:0.16591677069664001, decouple_loss:0.09129771590232849
/work/09012/haoli1/ls6/Pyt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Iteration: 2, ims.shape: (3, 48, 3, 720, 1440)
The modified gen p mean: 0.4477488100528717, gen p mean: -0.0007755590486340225, mean p set: 0.4482291042804718
The modified gen p mean: 0.4471951127052307, gen p mean: -0.0006091451505199075, mean p set: 0.4476754069328308
loss_pred:0.17348402738571167, decouple_loss:0.2725180387496948
loss_pred:0.17140868306159973, decouple_loss:0.2719382047653198
The modified gen p mean: 0.4489459991455078, gen p mean: -0.0006414279923774302, mean p set: 0.4494262933731079
loss_pred:0.16437388956546783, decouple_loss:0.2740817368030548
Traceback (most recent call last):
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/run1.py", line 338, in <module>
    train_wrapper(model)
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/run1.py", line 277, in train_wrapper
    trainer.train(model, ims, real_input_flag, args, itr)
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/core/trainer.py", line 25, in train
    cost = model.train(ims, real_input_flag)
  File "/work/09012/haoli1/ls6/ERA5_PredRNN/predrnn-pytorch/core/models/model_factory_multiGPU.py", line 63, in train
    loss.mean().backward()
  File "/work/09012/haoli1/ls6/Pyt/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/work/09012/haoli1/ls6/Pyt/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 3.61 GiB (GPU 0; 39.42 GiB total capacity; 32.38 GiB already allocated; 3.14 GiB free; 34.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF